{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "hqyAOWaAGCfx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DAG0X_1BF0dF"
      },
      "outputs": [],
      "source": [
        "# contoh teks\n",
        "teks = 'jln jatibaru polisi tdk bs geak gubernur emangny polisi tdk pmbhasan jgn berpolitik pengaturan wilayah hak gubernur tn abang turun temurun pelik kesabaran name name url'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NLTK"
      ],
      "metadata": {
        "id": "QmWncdc8GIGG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zh7uP8GSF2qW",
        "outputId": "71d5d996-351e-47db-9570-2e4b4e0fa8e1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "def word_tokenization(text):\n",
        "    # Kode untuk melakukan word tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "3nywBCzxI08P"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### WordTokenizer"
      ],
      "metadata": {
        "id": "fgqbmSq-GLkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "def word_tokenization(text):\n",
        "    # Kode untuk melakukan word tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "fRISfgEJF7Wx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenization = word_tokenization(teks)\n",
        "print(tokenization)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaXlVaesF74k",
        "outputId": "7d539deb-c9cc-4aed-edb6-5bee12940295"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['jln', 'jatibaru', 'polisi', 'tdk', 'bs', 'geak', 'gubernur', 'emangny', 'polisi', 'tdk', 'pmbhasan', 'jgn', 'berpolitik', 'pengaturan', 'wilayah', 'hak', 'gubernur', 'tn', 'abang', 'turun', 'temurun', 'pelik', 'kesabaran', 'name', 'name', 'url']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentence Tokenizer"
      ],
      "metadata": {
        "id": "3SHkal6kGV5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "teks = 'Saya senang bermain bola. Adik saya senang bermain basket.'"
      ],
      "metadata": {
        "id": "KrhD1sl9GwRe"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "def sentence_tokenization(text):\n",
        "    # Kode untuk melakukan sentence tokenization\n",
        "    sentences = sent_tokenize(text)\n",
        "    return sentences"
      ],
      "metadata": {
        "id": "IKYWZFWmF9DQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_tokenization = sentence_tokenization(teks)\n",
        "print(sent_tokenization)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfzPrTM3GZXx",
        "outputId": "d17dee84-a853-4fcd-838e-77a35eaf4f5c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Saya senang bermain bola.', 'Adik saya senang bermain basket.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tweet Tokenizer"
      ],
      "metadata": {
        "id": "0DV5RnocHGyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "teks = 'Drama kmarin sore : seharian puasa trus pas lg mandi kaget bcz kedatangan tamu tak diundang pas bgt woi pas lagi adzan maghrib trus jd galau dong seharian itu dianggap sah puasa apa enggak-_-'"
      ],
      "metadata": {
        "id": "yjhp2_WTGoSJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "def twet_tokenization(text):\n",
        "    # Kode untuk melakukan sentence tokenization\n",
        "    tweet = TweetTokenizer()\n",
        "    text_x = tweet.tokenize(text)\n",
        "    return text_x"
      ],
      "metadata": {
        "id": "G9dPCEKmHKic"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_tokenization = twet_tokenization(text=teks)\n",
        "print(x_tokenization)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZxsugjEHWFa",
        "outputId": "1dcd228e-09f5-41e1-f56b-41cc88c01508"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Drama', 'kmarin', 'sore', ':', 'seharian', 'puasa', 'trus', 'pas', 'lg', 'mandi', 'kaget', 'bcz', 'kedatangan', 'tamu', 'tak', 'diundang', 'pas', 'bgt', 'woi', 'pas', 'lagi', 'adzan', 'maghrib', 'trus', 'jd', 'galau', 'dong', 'seharian', 'itu', 'dianggap', 'sah', 'puasa', 'apa', 'enggak', '-', '_', '-']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### N-Gram"
      ],
      "metadata": {
        "id": "s7e3yXHnISeu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "teks = 'Drama kmarin sore : seharian puasa trus pas lg mandi kaget bcz kedatangan tamu tak diundang pas bgt woi pas lagi adzan maghrib trus jd galau dong seharian itu dianggap sah puasa apa enggak-_-'"
      ],
      "metadata": {
        "id": "J90KoSpWHefi"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ngram_tokenization(text, n):\n",
        "    # Kode untuk melakukan n-gram tokenization\n",
        "    tokens = nltk.ngrams(text.split(), n)\n",
        "    return list(tokens)"
      ],
      "metadata": {
        "id": "9dY8SBHEIWYW"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ngram_tokenized = ngram_tokenization(teks, 4)\n",
        "print(ngram_tokenized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFo-QspfIZ0x",
        "outputId": "ff17c9f2-f532-40c4-8556-4a1d9761ef2c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Drama', 'kmarin', 'sore', ':'), ('kmarin', 'sore', ':', 'seharian'), ('sore', ':', 'seharian', 'puasa'), (':', 'seharian', 'puasa', 'trus'), ('seharian', 'puasa', 'trus', 'pas'), ('puasa', 'trus', 'pas', 'lg'), ('trus', 'pas', 'lg', 'mandi'), ('pas', 'lg', 'mandi', 'kaget'), ('lg', 'mandi', 'kaget', 'bcz'), ('mandi', 'kaget', 'bcz', 'kedatangan'), ('kaget', 'bcz', 'kedatangan', 'tamu'), ('bcz', 'kedatangan', 'tamu', 'tak'), ('kedatangan', 'tamu', 'tak', 'diundang'), ('tamu', 'tak', 'diundang', 'pas'), ('tak', 'diundang', 'pas', 'bgt'), ('diundang', 'pas', 'bgt', 'woi'), ('pas', 'bgt', 'woi', 'pas'), ('bgt', 'woi', 'pas', 'lagi'), ('woi', 'pas', 'lagi', 'adzan'), ('pas', 'lagi', 'adzan', 'maghrib'), ('lagi', 'adzan', 'maghrib', 'trus'), ('adzan', 'maghrib', 'trus', 'jd'), ('maghrib', 'trus', 'jd', 'galau'), ('trus', 'jd', 'galau', 'dong'), ('jd', 'galau', 'dong', 'seharian'), ('galau', 'dong', 'seharian', 'itu'), ('dong', 'seharian', 'itu', 'dianggap'), ('seharian', 'itu', 'dianggap', 'sah'), ('itu', 'dianggap', 'sah', 'puasa'), ('dianggap', 'sah', 'puasa', 'apa'), ('sah', 'puasa', 'apa', 'enggak-_-')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word tokenizer NLP_ID (Kumparan)"
      ],
      "metadata": {
        "id": "Xmd1SdDfJAog"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nlp_id.tokenizer import Tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "teks_token = tokenizer.tokenize(teks)\n",
        "print(\"Teks word tokenize : \", teks_token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GjjYSOLIpUw",
        "outputId": "c0d29fe1-ef14-4585-bd2c-7d66e6a11245"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teks word tokenize :  ['jln', 'jatibaru', 'polisi', 'tdk', 'bs', 'geak', 'gubernur', 'emangny', 'polisi', 'tdk', 'pmbhasan', 'jgn', 'berpolitik', 'pengaturan', 'wilayah', 'hak', 'gubernur', 'tn', 'abang', 'turun', 'temurun', 'pelik', 'kesabaran', 'name', 'name', 'url']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TDOqG559JBOJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}