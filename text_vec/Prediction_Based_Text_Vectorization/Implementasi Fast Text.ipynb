{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('dataset\\Twitter_Emotion_Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4401 entries, 0 to 4400\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   label   4401 non-null   object\n",
      " 1   tweet   4401 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 68.9+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = dataset['tweet'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Funda NLP\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from text_preprocessing.asycro_text_processor import TextProcessor  \n",
    "\n",
    "BATCH_SIZE = 1000\n",
    "\n",
    "async def process_batch(batch):\n",
    "    processor = TextProcessor()\n",
    "    return await processor.process_texts(batch)\n",
    "\n",
    "async def process_large_dataset(texts):\n",
    "    results = []\n",
    "    for i in range(0, len(texts), BATCH_SIZE):\n",
    "        batch = texts[i:i + BATCH_SIZE]\n",
    "        batch_results = await process_batch(batch)\n",
    "        results.extend(batch_results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaned_texts = asyncio.run(process_large_dataset(texts))\n",
    "cleaned_texts = await process_large_dataset(texts)\n",
    "\n",
    "# Simpan hasil ke dataframe\n",
    "dataset['cleaned_text'] = cleaned_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cewek lho kayak rasain sibuk jaga rasain sakit haid panik pulang malam orang asing wajar korban takut curhat bela hujat'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['cleaned_text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anger</td>\n",
       "      <td>Soal jln Jatibaru,polisi tdk bs GERTAK gubernu...</td>\n",
       "      <td>jalan jatibaru polisi geak gubernur emangny po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anger</td>\n",
       "      <td>Sesama cewe lho (kayaknya), harusnya bisa lebi...</td>\n",
       "      <td>cewek lho kayak rasain sibuk jaga rasain sakit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>happy</td>\n",
       "      <td>Kepingin gudeg mbarek Bu hj. Amad Foto dari go...</td>\n",
       "      <td>gudeg mbarek hj amad foto google sengaja biar ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>anger</td>\n",
       "      <td>Jln Jatibaru,bagian dari wilayah Tn Abang.Peng...</td>\n",
       "      <td>jalan jatibaru wilayah tn abang atur wilayah t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>happy</td>\n",
       "      <td>Sharing pengalaman aja, kemarin jam 18.00 bata...</td>\n",
       "      <td>sharing alam kemarin jam batalin tiket stasiun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4396</th>\n",
       "      <td>love</td>\n",
       "      <td>Tahukah kamu, bahwa saat itu papa memejamkan m...</td>\n",
       "      <td>papa mejam mata tahan gejolak batin papa tapu ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4397</th>\n",
       "      <td>fear</td>\n",
       "      <td>Sulitnya menetapkan Calon Wapresnya Jokowi di ...</td>\n",
       "      <td>sulit calon wapresnya jokowi pilpres salah gem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4398</th>\n",
       "      <td>anger</td>\n",
       "      <td>5. masa depannya nggak jelas. lha iya, gimana ...</td>\n",
       "      <td>lha coba lulus seni nari kerja nari nari hasil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4399</th>\n",
       "      <td>happy</td>\n",
       "      <td>[USERNAME] dulu beneran ada mahasiswa Teknik U...</td>\n",
       "      <td>name beneran mahasiswa teknik ui nyata cinta p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4400</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Ya Allah, hanya Engkau yang mengetahui rasa sa...</td>\n",
       "      <td>allah engkau sakit hati sembuh allah</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4401 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        label                                              tweet  \\\n",
       "0       anger  Soal jln Jatibaru,polisi tdk bs GERTAK gubernu...   \n",
       "1       anger  Sesama cewe lho (kayaknya), harusnya bisa lebi...   \n",
       "2       happy  Kepingin gudeg mbarek Bu hj. Amad Foto dari go...   \n",
       "3       anger  Jln Jatibaru,bagian dari wilayah Tn Abang.Peng...   \n",
       "4       happy  Sharing pengalaman aja, kemarin jam 18.00 bata...   \n",
       "...       ...                                                ...   \n",
       "4396     love  Tahukah kamu, bahwa saat itu papa memejamkan m...   \n",
       "4397     fear  Sulitnya menetapkan Calon Wapresnya Jokowi di ...   \n",
       "4398    anger  5. masa depannya nggak jelas. lha iya, gimana ...   \n",
       "4399    happy  [USERNAME] dulu beneran ada mahasiswa Teknik U...   \n",
       "4400  sadness  Ya Allah, hanya Engkau yang mengetahui rasa sa...   \n",
       "\n",
       "                                           cleaned_text  \n",
       "0     jalan jatibaru polisi geak gubernur emangny po...  \n",
       "1     cewek lho kayak rasain sibuk jaga rasain sakit...  \n",
       "2     gudeg mbarek hj amad foto google sengaja biar ...  \n",
       "3     jalan jatibaru wilayah tn abang atur wilayah t...  \n",
       "4     sharing alam kemarin jam batalin tiket stasiun...  \n",
       "...                                                 ...  \n",
       "4396  papa mejam mata tahan gejolak batin papa tapu ...  \n",
       "4397  sulit calon wapresnya jokowi pilpres salah gem...  \n",
       "4398  lha coba lulus seni nari kerja nari nari hasil...  \n",
       "4399  name beneran mahasiswa teknik ui nyata cinta p...  \n",
       "4400               allah engkau sakit hati sembuh allah  \n",
       "\n",
       "[4401 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAST TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import gzip\n",
    "import numpy as np\n",
    "from keras.layers import TextVectorization\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat embedding matrix\n",
    "def create_embedding_matrix(vocab_and_vectors, word_index, embedding_dim):\n",
    "    \"\"\"\n",
    "    Membuat matriks embedding berdasarkan model embedding dan indeks kata.\n",
    "\n",
    "    Args:\n",
    "        vocab_and_vectors (dict): Peta kata ke vektor embedding dari model FastText.\n",
    "        word_index (dict): Peta kata ke indeks kata dari hasil tokenisasi.\n",
    "        embedding_dim (int): Dimensi vektor embedding.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Matriks embedding dengan bentuk (jumlah_kata, dimensi_embedding).\n",
    "    \"\"\"\n",
    "    num_words = len(word_index) + 1  # Jumlah kata termasuk indeks 0\n",
    "    embedding_matrix = np.zeros((num_words, embedding_dim))  # Inisialisasi matriks dengan 0\n",
    "\n",
    "    for word, i in word_index.items():\n",
    "        if i >= num_words:\n",
    "            continue  # Lewati jika indeks lebih dari jumlah kata\n",
    "        vector = vocab_and_vectors.get(word)  # Ambil vektor kata\n",
    "        if vector is not None:\n",
    "            embedding_matrix[i] = vector  # Isi matriks embedding dengan vektor kata\n",
    "    return embedding_matrix\n",
    "\n",
    "# Dimensi embedding FastText\n",
    "embedding_dim = 300\n",
    "\n",
    "# Tokenize and pad the text data\n",
    "max_vocab_size = 10000  # Jumlah maksimal kata dalam vocab\n",
    "max_seq_length = 400    # Panjang maksimum urutan\n",
    "\n",
    "# Inisialisasi TextVectorization\n",
    "vectorizer = TextVectorization(\n",
    "    max_tokens=max_vocab_size,\n",
    "    output_sequence_length=max_seq_length\n",
    ")\n",
    "\n",
    "# Fit TextVectorization pada data teks\n",
    "text_data = dataset['cleaned_text']\n",
    "vectorizer.adapt(text_data)\n",
    "\n",
    "\n",
    "# Mendownload model FastText untuk bahasa Indonesia\n",
    "url = 'https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.id.300.vec.gz'\n",
    "\n",
    "with gzip.open(urlopen(url), 'rt', encoding='utf-8') as file:\n",
    "    vocab_and_vectors = {}  # memetakan kata ke vektor\n",
    "    for line in file:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        vocab_and_vectors[word] = vector\n",
    "\n",
    "# Mendapatkan daftar vocabulary dari TextVectorization\n",
    "vocabulary = vectorizer.get_vocabulary()\n",
    "\n",
    "# Membuat word_index (dictionary kata ke indeks)\n",
    "word_index = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "\n",
    "# Membuat embedding matrix\n",
    "embedding_matrix = create_embedding_matrix(vocab_and_vectors, word_index, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 10000\n"
     ]
    }
   ],
   "source": [
    "# Count unique words\n",
    "num_unique_words = len(vectorizer.get_vocabulary())\n",
    "print(\"Number of unique words:\", num_unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text 1:\n",
      "jalan jatibaru polisi geak gubernur emangny polisi pmbhasan politik atur wilayah hak gubernur tn abang turun turun pelik sabar name name url\n",
      "Indexed Sequence 1:\n",
      "[  19 4888  482    1  473    1  482 7712  186  372 1418  407  473 3791\n",
      "  412  264  264 7887  150    2    2    4    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0]\n",
      "Original Text 2:\n",
      "cewek lho kayak rasain sibuk jaga rasain sakit haid panik pulang malam orang asing wajar korban takut curhat bela hujat\n",
      "Indexed Sequence 2:\n",
      "[  66  444   34 1297  288  147 1297   32 1798  732   49   39    3 1139\n",
      "  494  317    7  662  386  814    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0]\n",
      "Original Text 3:\n",
      "gudeg mbarek hj amad foto google sengaja biar teman teman bayang indah\n",
      "Indexed Sequence 3:\n",
      "[   1 9010    1    1   99 1107  554   28   10   10  311  111    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# Contoh teks dan sequence indexing\n",
    "for i, text in enumerate(text_data[:3]):  # Ambil 3 contoh pertama\n",
    "    indexed_sequence = vectorizer(text).numpy()  # Hasil sequence indexing\n",
    "    print(f\"Original Text {i + 1}:\\n{text}\")\n",
    "    print(f\"Indexed Sequence {i + 1}:\\n{indexed_sequence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Padded Sequence 1:\n",
      "[  19 4888  482    1  473    1  482 7712  186  372 1418  407  473 3791\n",
      "  412  264  264 7887  150    2    2    4    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0]\n",
      "\n",
      "Padded Sequence 2:\n",
      "[  66  444   34 1297  288  147 1297   32 1798  732   49   39    3 1139\n",
      "  494  317    7  662  386  814    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0]\n",
      "\n",
      "Padded Sequence 3:\n",
      "[   1 9010    1    1   99 1107  554   28   10   10  311  111    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Tokenisasi dan padding teks\n",
    "sequences = [vectorizer(text).numpy() for text in text_data[:3]]\n",
    "X_padded = pad_sequences(sequences, maxlen=max_seq_length, padding='post')\n",
    "\n",
    "for i, padded_seq in enumerate(X_padded):\n",
    "    print(f\"\\nPadded Sequence {i + 1}:\\n{padded_seq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example Word Embedding for '[UNK]':\n",
      "None\n",
      "\n",
      "Example Word Embedding for 'name':\n",
      "[-4.790e-02  1.000e-02 -1.570e-02 -1.430e-02 -1.513e-01 -6.690e-02\n",
      " -2.250e-02  3.040e-02  6.500e-03 -9.960e-02 -4.380e-02  1.064e-01\n",
      " -9.720e-02  1.960e-02  1.995e-01  2.760e-02  1.250e-01 -5.600e-03\n",
      "  5.900e-03  1.171e-01 -2.657e-01  1.440e-02 -1.500e-02  2.545e-01\n",
      "  7.250e-02 -2.690e-02  1.800e-03  2.870e-02  5.480e-02  1.357e-01\n",
      "  3.789e-01 -1.700e-03  1.781e-01  1.300e-01 -5.900e-02  8.300e-02\n",
      " -1.071e-01 -4.400e-02  2.202e-01 -5.480e-02  1.088e-01  1.042e-01\n",
      " -6.490e-02 -8.110e-02 -7.120e-02 -2.000e-04  1.467e-01  1.520e-02\n",
      "  9.740e-02 -2.310e-02 -1.160e-02  1.340e-01 -1.491e-01 -5.080e-02\n",
      "  1.472e-01  1.311e-01 -4.210e-02  2.110e-02  1.080e-01 -6.340e-02\n",
      "  6.160e-02 -7.160e-02 -1.300e-02  5.080e-02  2.351e-01  3.580e-02\n",
      " -4.950e-02  1.411e-01  2.060e-01  8.100e-03  4.830e-02 -4.730e-02\n",
      "  7.500e-02 -9.390e-02 -4.080e-02  9.160e-02 -1.749e-01 -1.360e-02\n",
      " -1.465e-01  3.000e-04 -1.250e-02 -8.030e-02  6.850e-02  6.260e-02\n",
      " -5.830e-02  1.090e-02 -7.180e-02 -1.816e-01  2.010e-02  1.720e-02\n",
      " -3.730e-02 -9.790e-02 -6.710e-02 -2.063e-01  1.545e-01 -2.143e-01\n",
      " -4.670e-02  1.800e-03  1.839e-01  1.112e-01  1.051e-01 -2.120e-02\n",
      " -9.250e-02  8.940e-02  1.264e-01  6.930e-02 -4.100e-03 -2.057e-01\n",
      " -4.760e-02  4.650e-02  1.356e-01 -3.022e-01  5.580e-02 -1.047e-01\n",
      "  5.290e-02  1.010e-01 -1.026e-01  4.250e-02  1.240e-02 -1.370e-01\n",
      " -8.990e-02 -3.780e-02  7.550e-02  5.650e-02 -4.530e-02 -1.171e-01\n",
      "  1.023e-01 -3.680e-02  9.810e-02 -3.290e-02 -1.890e-01  2.770e-02\n",
      "  5.320e-02 -2.480e-02 -2.998e-01  2.182e-01  1.032e-01  1.442e-01\n",
      " -4.980e-02  7.900e-03 -1.203e-01 -6.480e-02 -3.730e-02 -1.270e-02\n",
      " -7.260e-02  5.790e-02  1.990e-02  1.270e-02 -3.500e-03  8.930e-02\n",
      "  5.330e-02  2.100e-03 -3.105e-01 -3.960e-02 -1.720e-02  1.479e-01\n",
      " -5.200e-02 -2.670e-02 -4.970e-02  3.410e-02 -9.970e-02  4.200e-03\n",
      "  2.413e-01  4.810e-02 -1.204e-01  1.693e-01  1.940e-02  1.570e-02\n",
      " -1.947e-01 -2.520e-02 -7.960e-02  7.500e-03  2.030e-02 -1.032e-01\n",
      " -5.120e-02  1.111e-01 -9.090e-02  2.122e-01 -2.360e-02  5.210e-02\n",
      " -1.467e-01  3.850e-02  1.020e-01 -3.230e-02 -5.270e-02  1.247e-01\n",
      "  5.920e-02  2.478e-01  3.260e-02 -5.920e-02  1.023e-01  2.700e-02\n",
      "  7.400e-02 -1.940e-02 -5.680e-02  1.472e-01 -7.410e-02  8.710e-02\n",
      " -1.410e-02 -5.980e-02 -6.180e-02  9.110e-02  7.530e-02  9.260e-02\n",
      "  1.310e-01  4.790e-02  1.884e-01 -2.220e-02  1.704e-01 -6.840e-02\n",
      " -2.106e-01 -3.324e-01 -1.850e-01 -3.270e-02 -2.470e-02 -3.890e-02\n",
      "  4.140e-02  6.000e-02 -8.800e-03 -1.181e-01  7.490e-02  4.770e-02\n",
      " -1.636e-01 -1.372e-01 -3.650e-02  4.050e-02 -4.487e-01  1.502e-01\n",
      "  1.570e-01 -9.470e-02 -2.070e-02 -1.656e-01  1.077e-01 -1.008e-01\n",
      " -8.700e-03 -1.563e-01 -8.350e-02 -8.010e-02 -2.030e-02  6.890e-02\n",
      "  1.473e-01 -2.895e-01 -2.680e-02 -1.862e-01  6.800e-02 -3.140e-02\n",
      " -1.013e-01 -1.650e-02  2.100e-03 -9.020e-02  2.400e-03 -1.774e-01\n",
      " -2.483e-01 -2.210e-02 -6.080e-02  1.926e-01 -6.810e-02 -2.740e-02\n",
      " -7.040e-02 -5.050e-02  2.700e-02 -8.400e-02  2.550e-02 -1.697e-01\n",
      "  1.024e-01 -1.050e-01  1.585e-01  9.540e-02  1.280e-02 -1.160e-02\n",
      " -5.390e-02 -3.600e-02  8.200e-03 -3.330e-02 -1.146e-01 -7.900e-03\n",
      " -1.552e-01  4.470e-02  1.374e-01  6.790e-02 -3.920e-02 -3.080e-02\n",
      "  6.750e-02 -1.093e-01  1.320e-02 -1.607e-01 -3.240e-02  3.580e-02\n",
      " -4.230e-02  8.430e-02 -2.750e-02  7.790e-02  5.590e-02 -6.460e-02\n",
      "  1.665e-01 -8.940e-02  1.019e-01  6.750e-02  1.874e-01  1.744e-01]\n",
      "\n",
      "Example Word Embedding for 'orang':\n",
      "[ 4.400e-02 -3.030e-02 -1.770e-02  1.200e-01 -8.120e-02 -1.315e-01\n",
      " -4.200e-03 -1.420e-02 -1.400e-03 -1.045e-01 -1.270e-02 -9.000e-04\n",
      "  7.800e-03  2.320e-02  2.270e-02 -1.750e-02  1.260e-02  1.100e-03\n",
      "  3.910e-02  1.780e-02 -2.640e-02 -2.170e-02  8.600e-03  1.240e-02\n",
      "  2.750e-02 -3.200e-02 -2.340e-02 -1.700e-03  7.060e-02 -2.470e-02\n",
      "  1.224e-01 -1.610e-02  7.070e-02 -5.980e-02  7.710e-02 -2.490e-02\n",
      "  8.400e-03 -2.680e-02  1.220e-02 -8.000e-04 -2.170e-02 -3.000e-03\n",
      " -5.700e-03 -1.930e-02 -4.900e-02  3.500e-02 -3.370e-02  2.610e-02\n",
      " -4.500e-02 -9.580e-02 -2.250e-02  4.160e-02 -5.250e-02 -4.340e-02\n",
      " -4.500e-03  6.230e-02 -6.230e-02 -4.400e-03 -6.590e-02 -3.600e-03\n",
      "  3.200e-03 -1.600e-02 -4.320e-02  1.330e-02 -9.800e-03 -3.210e-02\n",
      "  1.090e-02  1.100e-03  6.400e-03 -5.800e-03  6.160e-02  4.780e-02\n",
      "  3.750e-02 -1.760e-02  3.000e-04  1.710e-02  3.950e-02 -4.590e-02\n",
      "  2.500e-03  7.300e-03  5.000e-03 -3.520e-02 -5.100e-03 -4.420e-02\n",
      " -2.920e-02 -9.700e-03 -1.526e-01  2.540e-02  2.090e-02 -2.300e-02\n",
      " -3.050e-02  3.790e-02  4.290e-02  7.600e-03 -1.287e-01 -1.349e-01\n",
      " -7.700e-03 -5.000e-04 -3.530e-02 -2.200e-02 -5.000e-03  6.100e-03\n",
      "  1.080e-02 -2.840e-02  4.210e-02  1.700e-02 -4.030e-02  1.820e-02\n",
      "  1.030e-02 -9.600e-03 -4.460e-02 -1.083e-01  4.230e-02  8.200e-03\n",
      " -5.770e-02 -3.280e-02  1.560e-02  2.860e-02  1.010e-02  7.700e-03\n",
      " -3.150e-02 -2.110e-02  2.550e-02 -3.030e-02  1.770e-02 -2.250e-02\n",
      "  1.570e-02  3.400e-03 -4.700e-02  1.470e-02 -8.900e-03 -1.180e-02\n",
      "  3.150e-02 -1.800e-03  9.900e-03 -3.000e-03 -2.500e-03 -8.700e-03\n",
      "  3.820e-02 -2.770e-02  4.490e-02 -1.288e-01  1.340e-02 -2.910e-02\n",
      " -1.270e-02  4.100e-02  1.370e-02 -3.050e-02  2.300e-03  3.000e-02\n",
      "  4.390e-02 -8.700e-03  1.990e-02  3.140e-02  7.930e-02  1.390e-02\n",
      " -3.000e-03  2.490e-02  2.260e-02 -3.720e-02  1.257e-01 -6.300e-03\n",
      " -5.800e-02 -1.530e-02 -3.080e-02  2.230e-02  2.460e-02 -3.920e-02\n",
      "  5.900e-03 -1.300e-02  1.360e-02 -1.840e-02  8.290e-02  2.310e-02\n",
      " -1.180e-02 -6.200e-03  1.000e-04  3.440e-02  1.930e-02  1.130e-02\n",
      "  3.380e-02  4.000e-04  1.100e-02 -1.500e-02  1.930e-02 -1.270e-02\n",
      "  3.990e-02  1.507e-01 -5.610e-02 -7.070e-02 -6.480e-02 -2.460e-02\n",
      " -5.000e-03  1.200e-03 -3.460e-02  3.500e-03 -6.400e-03  3.410e-02\n",
      " -2.080e-02  1.444e-01  1.760e-02 -3.900e-03 -2.800e-03 -1.420e-02\n",
      " -3.600e-03  3.200e-02 -2.320e-02  2.520e-02  2.410e-02  3.950e-02\n",
      " -1.460e-02  3.400e-03  1.970e-02  5.700e-03 -1.510e-02 -1.920e-02\n",
      " -1.810e-02  1.150e-02  2.490e-02 -4.800e-02  2.600e-03  2.110e-02\n",
      " -3.000e-03  2.000e-02 -4.730e-02  7.600e-03 -4.790e-02  7.600e-03\n",
      "  4.100e-03 -2.990e-02  4.700e-02  4.800e-03 -1.870e-02  2.640e-02\n",
      " -2.140e-02 -9.300e-03  1.900e-02  8.700e-03  9.900e-03  7.500e-03\n",
      "  3.200e-02  3.670e-02  1.940e-02 -2.720e-02  3.180e-02 -8.800e-03\n",
      " -5.190e-02  4.510e-02 -1.150e-02 -3.280e-02  5.370e-02 -2.990e-02\n",
      "  5.800e-02 -7.360e-02  4.800e-03 -1.310e-02 -2.800e-02 -3.120e-02\n",
      "  1.800e-03 -7.020e-02 -5.050e-02 -3.330e-02 -6.000e-04 -8.700e-03\n",
      " -3.110e-02 -1.780e-02 -3.320e-02  1.000e-02  2.620e-02  6.800e-03\n",
      "  1.270e-02 -8.400e-03 -2.400e-03 -4.170e-02 -8.590e-02  9.900e-03\n",
      " -9.670e-02  6.390e-02  6.150e-02  5.000e-04 -7.130e-02  1.520e-02\n",
      "  1.690e-02 -8.180e-02  3.480e-02  3.340e-02  2.150e-02 -5.600e-03\n",
      " -1.600e-03  3.200e-02  5.000e-03 -9.700e-03 -2.530e-02 -1.210e-02\n",
      " -8.500e-03  1.370e-02  2.500e-03 -4.630e-02 -2.320e-02  1.669e-01]\n"
     ]
    }
   ],
   "source": [
    "# Contoh embedding dari kata dalam vocabulary\n",
    "example_words = list(word_index.keys())[:3]  # Ambil 3 kata pertama dari word_index\n",
    "\n",
    "for word in example_words:\n",
    "    example_embedding = vocab_and_vectors.get(word)  # Ambil embedding dari FastText\n",
    "    print(f\"\\nExample Word Embedding for '{word}':\\n{example_embedding}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similar Words to 'jalan':\n",
      "jalan: 1.0000\n",
      "jalur: 0.6760\n",
      "setapak: 0.6255\n",
      "tol: 0.6167\n",
      "raya: 0.6082\n",
      "pinggir: 0.5978\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def find_similar_words(word, embedding_matrix, word_index, top_n=5):\n",
    "    \"\"\"\n",
    "    Temukan kata-kata yang mirip berdasarkan embedding matrix.\n",
    "    \n",
    "    Args:\n",
    "        word (str): Kata yang ingin dicari kemiripannya.\n",
    "        embedding_matrix (np.ndarray): Matriks embedding.\n",
    "        word_index (dict): Peta kata ke indeks kata.\n",
    "        top_n (int): Jumlah kata mirip yang ingin diambil.\n",
    "    \n",
    "    Returns:\n",
    "        list of tuple: Daftar kata mirip dan skor kemiripannya.\n",
    "    \"\"\"\n",
    "    if word in word_index:\n",
    "        idx = word_index[word]\n",
    "        embedding = embedding_matrix[idx].reshape(1, -1)  # Ubah ke array 2D untuk cosine similarity\n",
    "        cosine_similarities = cosine_similarity(embedding, embedding_matrix)\n",
    "        similar_words = [(word, 1.0)]  # Tambahkan kata input dengan similarity 1.0\n",
    "        for idx_similar in cosine_similarities.argsort()[0][-top_n-1:-1][::-1]:\n",
    "            similar_word = next(key for key, value in word_index.items() if value == idx_similar)\n",
    "            similarity = cosine_similarities[0][idx_similar]\n",
    "            similar_words.append((similar_word, similarity))\n",
    "        return similar_words\n",
    "    else:\n",
    "        print(f\"Word '{word}' not found in vocabulary.\")\n",
    "        return []\n",
    "\n",
    "# Contoh penggunaan: Cari kata mirip dengan \"jalan\"\n",
    "similar_words = find_similar_words(\"jalan\", embedding_matrix, word_index, top_n=5)\n",
    "\n",
    "print(\"\\nSimilar Words to 'jalan':\")\n",
    "for word, similarity in similar_words:\n",
    "    print(f\"{word}: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method TextVectorization.get_vocabulary of <TextVectorization name=text_vectorization_3, built=True>>\n"
     ]
    }
   ],
   "source": [
    "vocabulary = vectorizer.get_vocabulary()\n",
    "example_word = list(word_index.keys())[0]\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
